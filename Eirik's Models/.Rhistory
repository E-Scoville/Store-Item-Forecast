layer_dense(units=3, input_shape=ncol(X.train)) %>%
layer_activation("relu") %>%
layer_dense(units=1) %>%
layer_activation("sigmoid")
## Define how to fit a NN
nn.model %>% compile(optimizer=optimizer_sgd(lr=0.01),
loss="binary_crossentropy",
metrics=c("Precision", "Recall", "Accuracy"))
## Now fit a NN
fit.history <- nn.model %>% fit(x=X.train,
y=y.train,
batch_size=32,
epochs=1000,
validation_split=0.1,
verbose=0)
dim(X.train)
dim(y.train)
## Define how to fit a NN
nn.model %>% compile(optimizer=optimizer_sgd(lr=0.01),
loss="binary_crossentropy",
metrics=c("Precision", "Recall"))
## Now fit a NN
fit.history <- nn.model %>% fit(x=X.train,
y=y.train,
batch_size=32,
epochs=1000,
validation_split=0.1,
verbose=0)
ncol(X.train)
## Define a NN
nn.model <- keras_model_sequential() #Initialize
nn.model %>%
layer_dense(units=6, input_shape=ncol(X.train)) %>%
layer_activation("relu") %>%
layer_dense(units=1) %>%
layer_activation("sigmoid")
## Define how to fit a NN
nn.model %>% compile(optimizer=optimizer_sgd(lr=0.01),
loss="binary_crossentropy",
metrics=c("Precision", "Recall"))
## Now fit a NN
fit.history <- nn.model %>% fit(x=X.train,
y=y.train,
batch_size=32,
epochs=1000,
validation_split=0.1,
verbose=0)
## Define a NN
nn.model <- keras_model_sequential() #Initialize
nn.model %>%
layer_dense(units=6, input_shape=ncol(X.train)) %>%
layer_activation("relu") %>%
layer_dense(units=10) %>%
layer_activation("sigmoid")
## Define how to fit a NN
nn.model %>% compile(optimizer=optimizer_sgd(lr=0.01),
loss="binary_crossentropy",
metrics=c("Precision", "Recall"))
## Now fit a NN
fit.history <- nn.model %>% fit(x=X.train,
y=y.train,
batch_size=32,
epochs=1000,
validation_split=0.1,
verbose=0)
pnorm(160,100,15)
1-pnorm(160,100,15)
1-pnorm(160,100,15) * 7800000000
(1-pnorm(160,100,15)) * 7800000000
(1-pnorm(160,100,15)) * (7800000000*.25)
rm(list=ls())
#load directly from chmullig.com
bdata <- read.csv("http://chmullig.com/wp-content/uploads/2012/06/births.csv")
#filter
bdata<-bdata[(bdata$births > 1000),]
bdata$smoothbirths <- bdata$births
bdata$smoothbirths[bdata$month==2 & bdata$day==29] <- bdata$births[bdata$month==2 & bdata$day==29]*4
bdata$order <- rank(bdata$month + bdata$day/100)
loess(smoothbirths ~ order, span=1, iter=6, data=bdata)
birthloess = loess(smoothbirths ~ order, bdata)
bdata$predict <- predict(birthloess, bdata$order)
bdata$flag <- ifelse(bdata$births - bdata$predict > sd(bdata$smoothbirths)*2.3, 3,
ifelse(bdata$births - bdata$predict < sd(bdata$smoothbirths)*-2.3, 1, 0))
#special days we might care about
bdata$flag[bdata$month==2 & bdata$day==14] <- 3
bdata$flag[bdata$month==10 & bdata$day==31] <- 1
monthstarts <- by(bdata$order, list(bdata$month), min)
plot(smoothbirths ~ order, data=bdata, xaxt="n", xlab="Day", ylab="Births", main="Births by Day of Year", type="l", lwd=0)
axis(1, at=monthstarts, labels=c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"),
tck=1, lwd=0.5, lty=2, col="grey", cex.axis=0.99)
title(sub="Source: National Vital Statistics Natality Data, as provided by Google BigQuery.", cex.sub=0.75, adj=1)
#add the mean line
abline(a=sum(bdata$births)/365.25, b=0, col="red", lwd=0.5)
#add the smoothed line
lines(bdata$order, bdata$predict, col="blue", lwd=0.5)
legend("bottomright", c("Births", "Smoothed", "Mean"), col=c("black", "blue", "red"), lty=1, lwd=2, bty="n")
#label some outliers
with(bdata, text(order[flag==1], births[flag==1], paste(month[flag==1], "/", day[flag==1]), pos=1, cex=1))
with(bdata, text(order[flag==3], births[flag==3], paste(month[flag==3], "/", day[flag==3]), pos=3, cex=1))
#write the actual line
lines(births ~ order, data=bdata, xaxt="n", xlab="Day", ylab="Births", type="l", lwd=1.5)
bdata$probs <- bdata$births / sum(bdata$births)
bdata$smoothprobs <- bdata$smoothbirths / sum(bdata$smoothbirths)
worldpop <- 7771737945
bdata[(bdata$smoothprobs == max(bdata$smoothprobs)),]
bdata$gradiant <- bdata$smoothprobs / max(bdata$smoothprobs)
hist(bdata$smoothprobs)
bdata[,c(1,2,9)]
worldpop * bdata$smoothprobs[bdata$month==10 & bdata$day==12]
worldpop * bdata$probs[bdata$month==10 & bdata$day==12]
worldpop * 0.01
61000/worldpop
hist(bdata$probs)
max(bdata$probs)
max(bdata$smoothprobs)
mean(bdata$probs)
bdata$probs[bdata$month==10 & bdata$day==12]
plot(smoothbirths ~ order, data=bdata, xaxt="n", xlab="Day", ylab="Births", main="Births by Day of Year", type="l", lwd=0)
axis(1, at=monthstarts, labels=c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"),
tck=1, lwd=0.5, lty=2, col="grey", cex.axis=0.99)
title(sub="Source: National Vital Statistics Natality Data, as provided by Google BigQuery.", cex.sub=0.75, adj=1)
#add the mean line
abline(a=sum(bdata$births)/365.25, b=0, col="red", lwd=0.5)
#add the smoothed line
lines(bdata$order, bdata$predict, col="blue", lwd=0.5)
legend("bottomright", c("Births", "Smoothed", "Mean"), col=c("black", "blue", "red"), lty=1, lwd=2, bty="n")
#label some outliers
with(bdata, text(order[flag==1], births[flag==1], paste(month[flag==1], "/", day[flag==1]), pos=1, cex=1))
with(bdata, text(order[flag==3], births[flag==3], paste(month[flag==3], "/", day[flag==3]), pos=3, cex=1))
#write the actual line
lines(births ~ order, data=bdata, xaxt="n", xlab="Day", ylab="Births", type="l", lwd=1.5)
bdata$probs <- bdata$births / sum(bdata$births)
bdata$smoothprobs <- bdata$smoothbirths / sum(bdata$smoothbirths)
worldpop <- 7771737945
bdata[(bdata$smoothprobs == max(bdata$smoothprobs)),]
bdata$gradiant <- bdata$smoothprobs / max(bdata$smoothprobs)
hist(bdata$smoothprobs)
bdata[,c(1,2,9)]
write.csv(bdata, file="C:\\Users\\Arathen\\Documents\\bdata.csv")
worldpop * bdata$smoothprobs[bdata$month==3 & bdata$day==18]
worldpop * bdata$smoothprobs[bdata$month==10 & bdata$day==12]
worldpop * bdata$probs[bdata$month==3 & bdata$day==18]
worldpop * bdata$probs[bdata$month==10 & bdata$day==12]
worldpop * 0.01
61000/worldpop
abline(v=300)
abline(v=286, col="red", lwd=0.5)
bdata<-bdata[(bdata$births > 1000),]
bdata$smoothbirths <- bdata$births
bdata$smoothbirths[bdata$month==2 & bdata$day==29] <- bdata$births[bdata$month==2 & bdata$day==29]*4
bdata$order <- rank(bdata$month + bdata$day/100)
loess(smoothbirths ~ order, span=1, iter=6, data=bdata)
birthloess = loess(smoothbirths ~ order, bdata)
bdata$predict <- predict(birthloess, bdata$order)
bdata$flag <- ifelse(bdata$births - bdata$predict > sd(bdata$smoothbirths)*2.3, 3,
ifelse(bdata$births - bdata$predict < sd(bdata$smoothbirths)*-2.3, 1, 0))
#special days we might care about
bdata$flag[bdata$month==2 & bdata$day==14] <- 3
bdata$flag[bdata$month==10 & bdata$day==31] <- 1
monthstarts <- by(bdata$order, list(bdata$month), min)
plot(smoothbirths ~ order, data=bdata, xaxt="n", xlab="Day", ylab="Births", main="Births by Day of Year", type="l", lwd=0)
axis(1, at=monthstarts, labels=c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"),
tck=1, lwd=0.5, lty=2, col="grey", cex.axis=0.99)
title(sub="Source: National Vital Statistics Natality Data, as provided by Google BigQuery.", cex.sub=0.75, adj=1)
#add the mean line
abline(a=sum(bdata$births)/365.25, b=0, col="red", lwd=0.5)
abline(v=286, col="red", lwd=0.5)
#add the smoothed line
lines(bdata$order, bdata$predict, col="blue", lwd=0.5)
legend("bottomright", c("Births", "Smoothed", "Mean"), col=c("black", "blue", "red"), lty=1, lwd=2, bty="n")
#label some outliers
with(bdata, text(order[flag==1], births[flag==1], paste(month[flag==1], "/", day[flag==1]), pos=1, cex=1))
with(bdata, text(order[flag==3], births[flag==3], paste(month[flag==3], "/", day[flag==3]), pos=3, cex=1))
plot(smoothbirths ~ order, data=bdata, xaxt="n", xlab="Day", ylab="Births", main="Births by Day of Year", type="l", lwd=0)
axis(1, at=monthstarts, labels=c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"),
tck=1, lwd=0.5, lty=2, col="grey", cex.axis=0.99)
title(sub="Source: National Vital Statistics Natality Data, as provided by Google BigQuery.", cex.sub=0.75, adj=1)
#add the mean line
abline(a=sum(bdata$births)/365.25, b=0, col="red", lwd=0.5)
abline(v=286, col="red", lwd=0.5, lt=2)
#add the smoothed line
lines(bdata$order, bdata$predict, col="blue", lwd=0.5)
legend("bottomright", c("Births", "Smoothed", "Mean"), col=c("black", "blue", "red"), lty=1, lwd=2, bty="n")
with(bdata, text(order[flag==1], births[flag==1], paste(month[flag==1], "/", day[flag==1]), pos=1, cex=1))
with(bdata, text(order[flag==3], births[flag==3], paste(month[flag==3], "/", day[flag==3]), pos=3, cex=1))
#write the actual line
lines(births ~ order, data=bdata, xaxt="n", xlab="Day", ylab="Births", type="l", lwd=1.5)
# Normalized probabilities (Z-scores)
bdata$normprobs <- (bdata$probs - mean(bdata$probs))/sd(bdata$probs)
hist(bdata$normprobs)
plot(normprobs ~ order, data=bdata, xaxt="n", xlab="Day", ylab="Births", main="Births by Day of Year", type="l", lwd=0)
axis(1, at=monthstarts, labels=c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"),
tck=1, lwd=0.5, lty=2, col="grey", cex.axis=0.99)
title(sub="Source: National Vital Statistics Natality Data, as provided by Google BigQuery.", cex.sub=0.75, adj=1)
#add the mean line
abline(a=sum(bdata$births)/365.25, b=0, col="red", lwd=0.5)
#add the mean line
abline(a=sum(bdata$normprobs)/365.25, b=0, col="red", lwd=0.5)
abline(v=286, col="red", lwd=0.5, lt=2)
bdata$normprobs[bdata$month==10 & bdata$day==12]
bdata$normprobs[bdata$month==12 & bdata$day==6]
rm(list=ls())
data <- read.csv("C:/Users/Arathen/Desktop/Music Analysis/Caleb-Music-Database.csv")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
str(data)
hist(data$Year)
data$Year <- as.numeric(data$Year)
data$Year
data <- read.csv("C:/Users/Arathen/Desktop/Music Analysis/Caleb-Music-Database.csv")
data$Year <- as.numeric(data$Year)
hist(data$Year)
hist(data$Year, bins=50)
hist(data$Year, breaks = 50)
hist(data$Year, breaks = 25)
mean(data$Rating)
data %>% group.by(Year) %>% mean(Rating)
library(tidyverse)
data %>% group.by(Year) %>% mean(Rating)
data %>% group.by(Year) %>% mean(Rating)
data %>% groupby(Year) %>% mean(Rating)
data %>% group_by(Year) %>% mean(Rating)
data %>% mean(Rating) %>% group_by(Year)
mean(data$Rating)
data %>% mean(Rating) #%>% group_by(Year)
data$Rating <- as.numeric(data$Rating)
mean(data$Rating)
data %>% mean(Rating) #%>% group_by(Year)
data %>%
group_by(Year) %>%
summarise_at(vars(Rating), funs(mean(., na.rm=TRUE)))
data %>%
group_by(Year) %>%
summarise_at(vars(Rating), funs(mean()))
summarise_at(vars(Rating), funs(mean)
summarise_at(vars(Rating), funs(mean(.))
summarise_at(Rating, funs(mean(.))
summarise_at(Rating, funs(mean(Rating))
data %>%
group_by(Year) %>%
summarise_at(.vars = names(.)['Rating'],.funs = c(mean="mean"))
str(data)
data %>%
group_by(Year) %>%
summarise_at(.vars = names(.)[5],.funs = c(mean="mean"))
rate.per.year <- data %>%
group_by(Year) %>%
summarise_at(.vars = names(.)[5],.funs = c(mean="mean"))
hist(rate.per.year)
hist(rate.per.year[2])
str(rate.per.year)
plot(rate.per.year$Year, rate.per.year$mean)
plot(rate.per.year$Year, rate.per.year$mean, ty="l")
plot(rate.per.year$Year, rate.per.year$mean, ty="h")
source('~/.active-rstudio-document')
max(rate.per.year)
rm(list=ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
data <- read.csv("C:/Users/Arathen/Desktop/Music Analysis/Caleb-Music-Database.csv")
data$Rating <- as.numeric(data$Rating)
data$Year <- as.numeric(data$Year)
hist(data$Year)
hist(data$Year, breaks = 25)
mean(data$Rating)
data %>% mean(Rating) #%>% group_by(Year)
rate.per.year <- data %>%
group_by(Year) %>%
summarise_at(.vars = names(.)[5],.funs = c(mean="mean"))
hist(rate.per.year[2])
plot(rate.per.year$Year, rate.per.year$mean, ty="h")
max(rate.per.year)
rate.per.year[which(rate.per.year$mean = max(rate.per.year$mean)),]
rate.per.year[which(rate.per.year$mean == max(rate.per.year$mean)),]
rate.per.year[which(rate.per.year$mean == min(rate.per.year$mean)),]
table(data$Genre)
data %>% count(Genre)
data %>% count(Genre) %>% sort(decreasing = TRUE)
data %>% count(Genre) %>% sort(.,decreasing = TRUE)
data %>% count(Genre) %>% sort(Genre,decreasing = TRUE)
data %>% count(Genre) %>% sort(,decreasing = TRUE)
data %>% count(Genre, sort=TRUE)
data %>% count(lcase(Genre), sort=TRUE)
data %>% count(tolower(Genre), sort=TRUE)
rm(list=ls())
setwd("C:/Users/Arathen/Desktop/Github Projects/Store-Item-Forecast/Eirik's Models")
# Load libraries
library(randomForest)
library(DataExplorer)
library(forecast)
library(tidyverse)
library(tseries)
library(ggplot2)
train <- read.csv("../train.csv")
test <- read.csv("../test.csv")
fulldata <- bind_rows(train, test)
# Do some transformations
fulldata$date <- as.Date(fulldata$date)
fulldata <- fulldata %>% mutate_at(vars(store, item), factor)
# Break into train and test again
train <- fulldata %>% filter(is.na(sales) == FALSE)
test <- fulldata %>% filter(is.na(sales) == TRUE)
# Create nested for loop to subset, train, and predict
for (s in 1:1) {
for (i in 1:1) {
# Subset
temp_train <- train %>% filter((store == s) & (item == i)) %>% select(date, sales)
temp_test <- test %>% filter((store == s) & (item == i)) %>% select(date, sales)
## Prepare to fit ARIMA model
ts_train <- ts(temp_train[,2], start=c(2013,1), frequency = 365)
# Decomposing data
decomp <- stl(ts_train, s.window="periodic")
deseasonal <- seasadj(decomp) # Remove seasonality
# Fit model
fit <- auto.arima(ts_train, seasonal=TRUE, max.p = 15)
# Predict
seas_fcast <- forecast(fit, h=90)
# Join predictions to subset of test data
temp_test$sales <- seas_fcast["mean"]
}
}
dim(temp_test)
head(temp_test)
temp_test
temp_test <- DataFrame(temp_test)
temp_test <- array(temp_test)
temp_test
temp_test[1,]
temp_test
seas_fcast["mean"][1]
c(seas_fcast["mean"])
# Create nested for loop to subset, train, and predict
for (s in 1:1) {
for (i in 1:1) {
# Subset
temp_train <- train %>% filter((store == s) & (item == i)) %>% select(date, sales)
temp_test <- test %>% filter((store == s) & (item == i)) %>% select(date, sales)
## Prepare to fit ARIMA model
ts_train <- ts(temp_train[,2], start=c(2013,1), frequency = 365)
# Decomposing data
decomp <- stl(ts_train, s.window="periodic")
deseasonal <- seasadj(decomp) # Remove seasonality
# Fit model
fit <- auto.arima(train['sales'], seasonal=TRUE, max.p = 15)
# Predict
seas_fcast <- forecast(fit, h=90)
# Join predictions to subset of test data
temp_test$sales <- seas_fcast["mean"]
}
}
# Create nested for loop to subset, train, and predict
for (s in 1:1) {
for (i in 1:1) {
# Subset
temp_train <- train %>% filter((store == s) & (item == i)) %>% select(date, sales)
temp_test <- test %>% filter((store == s) & (item == i)) %>% select(date, sales)
## Prepare to fit ARIMA model
ts_train <- ts(temp_train[,2], start=c(2013,1), frequency = 365)
# Decomposing data
decomp <- stl(ts_train, s.window="periodic")
deseasonal <- seasadj(decomp) # Remove seasonality
# Fit model
fit <- auto.arima(ts_train, seasonal=TRUE, max.p = 15)
# Predict
seas_fcast <- forecast(fit, h=90)
# Join predictions to subset of test data
temp_test$sales <- seas_fcast["mean"]
}
}
train <- read.csv("../train.csv")
test <- read.csv("../test.csv")
fulldata <- bind_rows(train, test)
# Do some transformations
fulldata$date <- as.Date(fulldata$date)
fulldata <- fulldata %>% mutate_at(vars(store, item), factor)
# Break into train and test again
train <- fulldata %>% filter(is.na(sales) == FALSE)
test <- fulldata %>% filter(is.na(sales) == TRUE)
# Create nested for loop to subset, train, and predict
for (s in 1:1) {
for (i in 1:1) {
# Subset
temp_train <- train %>% filter((store == s) & (item == i)) %>% select(date, sales)
temp_test <- test %>% filter((store == s) & (item == i)) %>% select(date, sales)
## Prepare to fit ARIMA model
ts_train <- ts(temp_train[,2], start=c(2013,1), frequency = 365)
# Decomposing data
#decomp <- stl(ts_train, s.window="periodic")
#deseasonal <- seasadj(decomp) # Remove seasonality
# Fit model
fit <- auto.arima(ts_train, seasonal=TRUE, max.p = 15)
# Predict
seas_fcast <- forecast(fit, h=90)
# Join predictions to subset of test data
temp_test$sales <- seas_fcast["mean"]
}
}
plot(fit)
fit
plot(seas_fcast)
accuracy(seas_fcast)
predict(fit, n.ahead = 90)
plot(predict(fit, n.ahead = 90))
seas_fcast[["mean"]]
# Create nested for loop to subset, train, and predict
for (s in 1:1) {
for (i in 1:1) {
# Subset
temp_train <- train %>% filter((store == s) & (item == i)) %>% select(date, sales)
temp_test <- test %>% filter((store == s) & (item == i)) %>% select(date, sales)
## Prepare to fit ARIMA model
ts_train <- ts(temp_train[,2], start=c(2013,1), frequency = 365)
# Decomposing data
#decomp <- stl(ts_train, s.window="periodic")
#deseasonal <- seasadj(decomp) # Remove seasonality
# Fit model
fit <- auto.arima(ts_train, seasonal=TRUE, max.p = 15)
# Predict
seas_fcast <- forecast(fit, h=90)
# Join predictions to subset of test data
temp_test$sales <- seas_fcast[["mean"]]
}
}
c(seas_fcast["mean"])
as.array(seas_fcast$mean)
as.vector(seas_fcast$mean)
# Create nested for loop to subset, train, and predict
for (s in 1:1) {
for (i in 1:1) {
# Subset
temp_train <- train %>% filter((store == s) & (item == i)) %>% select(date, sales)
temp_test <- test %>% filter((store == s) & (item == i)) %>% select(date, sales)
## Prepare to fit ARIMA model
ts_train <- ts(temp_train[,2], start=c(2013,1), frequency = 365)
# Decomposing data
#decomp <- stl(ts_train, s.window="periodic")
#deseasonal <- seasadj(decomp) # Remove seasonality
# Fit model
fit <- auto.arima(ts_train, seasonal=TRUE, max.p = 15)
# Predict
seas_fcast <- forecast(fit, h=90)
# Join predictions to subset of test data
temp_test$sales <- as.vector(seas_fcast$mean)
}
}
dim(temp_test)
head(temp_test)
# Initialize dataframe to save all predictions
preds <- data.frame()
# Initialize dataframe to save all predictions
preds <- data.frame()
# Create nested for loop to subset, train, and predict
for (s in 1:1) {
for (i in 1:1) {
# Subset
temp_train <- train %>% filter((store == s) & (item == i)) %>% select(date, sales)
temp_test <- test %>% filter((store == s) & (item == i))
## Prepare to fit ARIMA model
ts_train <- ts(temp_train[,2], start=c(2013,1), frequency = 365)
# Decomposing data
#decomp <- stl(ts_train, s.window="periodic")
#deseasonal <- seasadj(decomp) # Remove seasonality
# Fit model
fit <- auto.arima(ts_train, seasonal=TRUE, max.p = 15)
# Predict
seas_fcast <- forecast(fit, h=90)
# Join predictions to subset of test data
temp_test$sales <- as.vector(seas_fcast$mean)
# Save predictions
preds <- rbind(preds, temp_test)
# Tracking
print(s, i)
}
}
dim(preds)
head(preds)
print(paste(s,i))
print(paste("Store number:", s, "Item number:", i))
fit <- auto.arima(ts_train, seasonal=TRUE, max.p = 15)
fit2 <- auto.arima(ts_train, seasonal=TRUE)
accuracy(fit)
accuracy(fit2)
plot(fit2)
plot(fit)
dim(preds)
head(preds)
# Initialize dataframe to save all predictions
preds <- data.frame()
# Create nested for loop to subset, train, and predict
for (s in 1:10) {
for (i in 1:50) {
# Subset
temp_train <- train %>% filter((store == s) & (item == i)) %>% select(date, sales)
temp_test <- test %>% filter((store == s) & (item == i))
## Prepare to fit ARIMA model
ts_train <- ts(temp_train[,2], start=c(2013,1), frequency = 365)
# Decomposing data
#decomp <- stl(ts_train, s.window="periodic")
#deseasonal <- seasadj(decomp) # Remove seasonality
# Fit model
fit <- auto.arima(ts_train, seasonal=TRUE)#, max.p = 15)
# Predict
seas_fcast <- forecast(fit, h=90)
# Join predictions to subset of test data
temp_test$sales <- as.vector(seas_fcast$mean)
# Save predictions
preds <- rbind(preds, temp_test)
# Tracking
print(paste("Store number:", s, "Item number:", i))
}
}
dim(preds)
head(preds)
